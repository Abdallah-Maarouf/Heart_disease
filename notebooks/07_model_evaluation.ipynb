{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Evaluation and Performance Analysis\n",
                "\n",
                "This notebook provides comprehensive evaluation of trained heart disease classification models including:\n",
                "- Performance metrics calculation\n",
                "- ROC curves and AUC analysis\n",
                "- Confusion matrices\n",
                "- Cross-validation assessment\n",
                "- Model comparison and recommendations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import sys\n",
                "import os\n",
                "sys.path.append('../src')\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "\n",
                "from model_evaluator import ModelEvaluator\n",
                "\n",
                "# Set up plotting\n",
                "plt.style.use('default')\n",
                "sns.set_palette(\"husl\")\n",
                "%matplotlib inline\n",
                "\n",
                "print(\"Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Initialize Model Evaluator and Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize the model evaluator\n",
                "evaluator = ModelEvaluator(random_state=42)\n",
                "\n",
                "# Load trained models and test data\n",
                "evaluator.load_models_and_data(\n",
                "    models_dir='../models/supervised',\n",
                "    data_path='../data/processed/heart_disease_selected.csv'\n",
                ")\n",
                "\n",
                "print(f\"\\nLoaded {len(evaluator.models)} models:\")\n",
                "for model_name in evaluator.models.keys():\n",
                "    print(f\"  - {model_name.replace('_', ' ').title()}\")\n",
                "\n",
                "print(f\"\\nTest set size: {len(evaluator.y_test)} samples\")\n",
                "print(f\"Class distribution: {dict(pd.Series(evaluator.y_test).value_counts().sort_index())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Calculate Classification Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate comprehensive metrics for all models\n",
                "print(\"Classification Metrics for All Models\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "all_metrics = {}\n",
                "for model_name, model in evaluator.models.items():\n",
                "    metrics = evaluator.calculate_classification_metrics(model_name, model)\n",
                "    all_metrics[model_name] = metrics\n",
                "    \n",
                "    print(f\"\\n{model_name.replace('_', ' ').title()}:\")\n",
                "    print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
                "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
                "    print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
                "    print(f\"  F1-Score:  {metrics['f1_score']:.4f}\")\n",
                "    print(f\"  ROC-AUC:   {metrics['roc_auc']:.4f}\")\n",
                "    print(f\"  Specificity: {metrics['specificity']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Performance Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create performance comparison table\n",
                "comparison_df = evaluator.compare_model_performance()\n",
                "print(\"Model Performance Comparison\")\n",
                "print(\"=\" * 30)\n",
                "print(comparison_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. ROC Curves Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate ROC curves\n",
                "roc_data = evaluator.generate_roc_curves(save_dir='../results/model_evaluation/plots')\n",
                "\n",
                "# Display ROC curve data\n",
                "print(\"ROC-AUC Scores:\")\n",
                "print(\"-\" * 20)\n",
                "for model_name, data in roc_data.items():\n",
                "    print(f\"{model_name.replace('_', ' ').title()}: {data['auc']:.4f}\")\n",
                "\n",
                "# Load and display the ROC curves plot\n",
                "from IPython.display import Image, display\n",
                "display(Image('../results/model_evaluation/plots/roc_curves_comparison.png'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Confusion Matrices"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate confusion matrices\n",
                "confusion_matrices = evaluator.plot_confusion_matrices(save_dir='../results/model_evaluation/plots')\n",
                "\n",
                "# Display confusion matrix data\n",
                "print(\"Confusion Matrices:\")\n",
                "print(\"-\" * 20)\n",
                "for model_name, cm in confusion_matrices.items():\n",
                "    print(f\"\\n{model_name.replace('_', ' ').title()}:\")\n",
                "    print(f\"  True Negatives:  {cm[0,0]}\")\n",
                "    print(f\"  False Positives: {cm[0,1]}\")\n",
                "    print(f\"  False Negatives: {cm[1,0]}\")\n",
                "    print(f\"  True Positives:  {cm[1,1]}\")\n",
                "\n",
                "# Display the confusion matrices plot\n",
                "display(Image('../results/model_evaluation/plots/confusion_matrices.png'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Cross-Validation Assessment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Perform cross-validation\n",
                "cv_results = evaluator.cross_validation_scores(cv_folds=5)\n",
                "\n",
                "# Display cross-validation results\n",
                "print(\"5-Fold Cross-Validation Results\")\n",
                "print(\"=\" * 35)\n",
                "\n",
                "cv_summary = []\n",
                "for model_name, results in cv_results.items():\n",
                "    cv_summary.append({\n",
                "        'Model': model_name.replace('_', ' ').title(),\n",
                "        'CV Accuracy': f\"{results['accuracy']['mean']:.4f} ± {results['accuracy']['std']:.4f}\",\n",
                "        'CV Precision': f\"{results['precision']['mean']:.4f} ± {results['precision']['std']:.4f}\",\n",
                "        'CV Recall': f\"{results['recall']['mean']:.4f} ± {results['recall']['std']:.4f}\",\n",
                "        'CV F1-Score': f\"{results['f1']['mean']:.4f} ± {results['f1']['std']:.4f}\"\n",
                "    })\n",
                "\n",
                "cv_df = pd.DataFrame(cv_summary)\n",
                "print(cv_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Precision-Recall Curves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate precision-recall curves\n",
                "pr_data = evaluator.plot_precision_recall_curves(save_dir='../results/model_evaluation/plots')\n",
                "\n",
                "# Display average precision scores\n",
                "print(\"Average Precision Scores:\")\n",
                "print(\"-\" * 25)\n",
                "for model_name, data in pr_data.items():\n",
                "    print(f\"{model_name.replace('_', ' ').title()}: {data['average_precision']:.4f}\")\n",
                "\n",
                "# Display the precision-recall curves plot\n",
                "display(Image('../results/model_evaluation/plots/precision_recall_curves.png'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Model Comparison Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create model comparison plots\n",
                "evaluator.plot_model_comparison_metrics(save_dir='../results/model_evaluation/plots')\n",
                "\n",
                "# Display the comparison plots\n",
                "print(\"Model Performance Metrics Comparison:\")\n",
                "display(Image('../results/model_evaluation/plots/metrics_comparison.png'))\n",
                "\n",
                "print(\"\\nROC-AUC Score Comparison:\")\n",
                "display(Image('../results/model_evaluation/plots/roc_auc_comparison.png'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Classification Reports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate detailed classification reports\n",
                "classification_reports = evaluator.generate_classification_report(save_dir='../results/model_evaluation')\n",
                "\n",
                "# Display classification reports\n",
                "for model_name, report in classification_reports.items():\n",
                "    print(f\"\\nClassification Report - {model_name.replace('_', ' ').title()}\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    # Create a formatted DataFrame from the report\n",
                "    report_df = pd.DataFrame(report).transpose()\n",
                "    \n",
                "    # Display the main classification metrics\n",
                "    print(report_df.loc[['No Disease', 'Disease', 'macro avg', 'weighted avg']].round(4))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Comprehensive Performance Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate comprehensive performance summary\n",
                "summary = evaluator.model_performance_summary(save_dir='../results/model_evaluation')\n",
                "\n",
                "print(\"Comprehensive Performance Summary\")\n",
                "print(\"=\" * 40)\n",
                "\n",
                "print(f\"\\nDataset Information:\")\n",
                "print(f\"  Test Samples: {summary['dataset_info']['test_samples']}\")\n",
                "print(f\"  Positive Class Ratio: {summary['dataset_info']['positive_class_ratio']:.3f}\")\n",
                "print(f\"  Class Distribution: {summary['dataset_info']['class_distribution']}\")\n",
                "\n",
                "print(f\"\\nBest Performing Models:\")\n",
                "for metric, info in summary['best_performing_models'].items():\n",
                "    print(f\"  {metric.upper()}: {info['model'].replace('_', ' ').title()} ({info['score']:.4f})\")\n",
                "\n",
                "print(f\"\\nOverall Best Model: {summary['recommendations']['overall_best_model']['model'].replace('_', ' ').title()}\")\n",
                "print(f\"Overall Score: {summary['recommendations']['overall_best_model']['score']:.4f}\")\n",
                "\n",
                "print(f\"\\nUse Case Recommendations:\")\n",
                "for use_case, rec in summary['recommendations']['use_case_recommendations'].items():\n",
                "    print(f\"  {use_case.replace('_', ' ').title()}: {rec['model'].replace('_', ' ').title()}\")\n",
                "    print(f\"    Reason: {rec['reason']}\")\n",
                "\n",
                "print(f\"\\nPerformance Insights:\")\n",
                "for insight in summary['recommendations']['performance_insights']:\n",
                "    print(f\"  • {insight}\")\n",
                "\n",
                "if summary['recommendations']['improvement_suggestions']:\n",
                "    print(f\"\\nImprovement Suggestions:\")\n",
                "    for suggestion in summary['recommendations']['improvement_suggestions']:\n",
                "        print(f\"  • {suggestion}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Save All Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save all evaluation results\n",
                "evaluator.save_evaluation_results('../results/model_evaluation/evaluation_metrics.json')\n",
                "\n",
                "print(\"All evaluation results saved successfully!\")\n",
                "print(\"\\nGenerated files:\")\n",
                "print(\"  - evaluation_metrics.json (comprehensive results)\")\n",
                "print(\"  - model_evaluation_summary.txt (human-readable summary)\")\n",
                "print(\"  - plots/ directory with all visualizations\")\n",
                "print(\"  - individual classification reports\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Model Selection Recommendations\n",
                "\n",
                "Based on the comprehensive evaluation, here are the key findings and recommendations:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display final recommendations\n",
                "print(\"FINAL MODEL RECOMMENDATIONS\")\n",
                "print(\"=\" * 30)\n",
                "\n",
                "# Find the best model based on different criteria\n",
                "best_accuracy = max(all_metrics.items(), key=lambda x: x[1]['accuracy'])\n",
                "best_f1 = max(all_metrics.items(), key=lambda x: x[1]['f1_score'])\n",
                "best_roc_auc = max(all_metrics.items(), key=lambda x: x[1]['roc_auc'])\n",
                "\n",
                "print(f\"\\n1. BEST OVERALL PERFORMANCE:\")\n",
                "print(f\"   Model: {best_f1[0].replace('_', ' ').title()}\")\n",
                "print(f\"   F1-Score: {best_f1[1]['f1_score']:.4f}\")\n",
                "print(f\"   Reason: Best balance of precision and recall\")\n",
                "\n",
                "print(f\"\\n2. HIGHEST ACCURACY:\")\n",
                "print(f\"   Model: {best_accuracy[0].replace('_', ' ').title()}\")\n",
                "print(f\"   Accuracy: {best_accuracy[1]['accuracy']:.4f}\")\n",
                "\n",
                "print(f\"\\n3. BEST PROBABILITY RANKING:\")\n",
                "print(f\"   Model: {best_roc_auc[0].replace('_', ' ').title()}\")\n",
                "print(f\"   ROC-AUC: {best_roc_auc[1]['roc_auc']:.4f}\")\n",
                "print(f\"   Reason: Best for ranking patients by disease risk\")\n",
                "\n",
                "print(f\"\\n4. CLINICAL CONSIDERATIONS:\")\n",
                "print(f\"   - For screening (high recall needed): Focus on models with recall > 0.85\")\n",
                "print(f\"   - For diagnosis confirmation (high precision needed): Focus on models with precision > 0.80\")\n",
                "print(f\"   - For risk assessment: Use models with high ROC-AUC (> 0.90)\")\n",
                "\n",
                "print(f\"\\nEvaluation completed successfully! Check the results directory for detailed outputs.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}