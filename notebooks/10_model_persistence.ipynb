{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Persistence and Pipeline Management\n",
                "\n",
                "This notebook demonstrates the complete model persistence system including:\n",
                "- Saving and loading complete ML pipelines\n",
                "- Model versioning and metadata management\n",
                "- Model validation and integrity checks\n",
                "- Deployment preparation\n",
                "- Performance benchmarking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "sys.path.append('../src')\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import joblib\n",
                "import json\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from datetime import datetime\n",
                "\n",
                "# Import our custom modules\n",
                "from model_persistence import ModelPersistence, create_production_model\n",
                "from data_processor import DataProcessor\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Set up plotting\n",
                "plt.style.use('default')\n",
                "sns.set_palette(\"husl\")\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Initialize Model Persistence System"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize the model persistence system\n",
                "persistence = ModelPersistence(base_dir=\"../models\", log_level=\"INFO\")\n",
                "\n",
                "print(\"Model Persistence System Initialized\")\n",
                "print(f\"Base directory: {persistence.base_dir}\")\n",
                "print(f\"Production directory: {persistence.production_dir}\")\n",
                "print(f\"Versions directory: {persistence.versions_dir}\")\n",
                "print(f\"Metadata directory: {persistence.metadata_dir}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load and Prepare Data for Demonstration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load processed data if available, otherwise create sample data\n",
                "try:\n",
                "    # Try to load existing processed data\n",
                "    train_data = pd.read_csv('../data/processed/heart_disease_train.csv')\n",
                "    test_data = pd.read_csv('../data/processed/heart_disease_test.csv')\n",
                "    \n",
                "    X_train = train_data.drop('target', axis=1)\n",
                "    y_train = train_data['target']\n",
                "    X_test = test_data.drop('target', axis=1)\n",
                "    y_test = test_data['target']\n",
                "    \n",
                "    print(f\"Loaded existing data: {X_train.shape[0]} training samples, {X_test.shape[0]} test samples\")\n",
                "    \n",
                "except FileNotFoundError:\n",
                "    # Create sample data for demonstration\n",
                "    print(\"Creating sample data for demonstration...\")\n",
                "    \n",
                "    # Load raw data\n",
                "    processor = DataProcessor()\n",
                "    data = processor.load_data('../data/raw/heart_disease.csv')\n",
                "    \n",
                "    # Process data\n",
                "    processed_data = processor.preprocess_data(test_size=0.2, random_state=42)\n",
                "    \n",
                "    X_train = processed_data['X_train']\n",
                "    X_test = processed_data['X_test']\n",
                "    y_train = processed_data['y_train']\n",
                "    y_test = processed_data['y_test']\n",
                "    \n",
                "    print(f\"Created sample data: {X_train.shape[0]} training samples, {X_test.shape[0]} test samples\")\n",
                "\n",
                "print(f\"Feature columns: {list(X_train.columns)}\")\n",
                "print(f\"Target distribution - Train: {y_train.value_counts().to_dict()}\")\n",
                "print(f\"Target distribution - Test: {y_test.value_counts().to_dict()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Train a Sample Model for Demonstration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train a sample Random Forest model\n",
                "print(\"Training a sample Random Forest model...\")\n",
                "\n",
                "# Create and train model\n",
                "model = RandomForestClassifier(\n",
                "    n_estimators=100,\n",
                "    max_depth=10,\n",
                "    min_samples_split=5,\n",
                "    min_samples_leaf=2,\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "model.fit(X_train, y_train)\n",
                "\n",
                "# Evaluate model\n",
                "train_accuracy = model.score(X_train, y_train)\n",
                "test_accuracy = model.score(X_test, y_test)\n",
                "\n",
                "print(f\"Model trained successfully!\")\n",
                "print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
                "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
                "\n",
                "# Create preprocessing pipeline\n",
                "preprocessing_pipeline = Pipeline([\n",
                "    ('scaler', StandardScaler())\n",
                "])\n",
                "\n",
                "# Fit preprocessing pipeline on training data\n",
                "preprocessing_pipeline.fit(X_train)\n",
                "\n",
                "print(\"Preprocessing pipeline created and fitted.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Save Complete Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the complete pipeline\n",
                "model_name = \"random_forest_demo\"\n",
                "\n",
                "# Prepare metadata\n",
                "metadata = {\n",
                "    'performance_metrics': {\n",
                "        'train_accuracy': train_accuracy,\n",
                "        'test_accuracy': test_accuracy,\n",
                "        'n_train_samples': len(X_train),\n",
                "        'n_test_samples': len(X_test),\n",
                "        'n_features': X_train.shape[1]\n",
                "    },\n",
                "    'training_info': {\n",
                "        'algorithm': 'RandomForestClassifier',\n",
                "        'hyperparameters': model.get_params(),\n",
                "        'training_time': datetime.now().isoformat(),\n",
                "        'data_source': 'heart_disease_dataset'\n",
                "    },\n",
                "    'source': 'demonstration_notebook'\n",
                "}\n",
                "\n",
                "# Save complete pipeline\n",
                "saved_files = persistence.save_complete_pipeline(\n",
                "    model=model,\n",
                "    preprocessing_pipeline=preprocessing_pipeline,\n",
                "    model_name=model_name,\n",
                "    metadata=metadata\n",
                ")\n",
                "\n",
                "print(\"Complete pipeline saved successfully!\")\n",
                "for file_type, file_path in saved_files.items():\n",
                "    print(f\"  {file_type}: {file_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Load and Validate Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the pipeline\n",
                "loaded_pipeline, loaded_metadata = persistence.load_pipeline(model_name)\n",
                "\n",
                "print(\"Pipeline loaded successfully!\")\n",
                "print(f\"Model type: {loaded_metadata.get('model_type', 'Unknown')}\")\n",
                "print(f\"Version: {loaded_metadata.get('version', 'Unknown')}\")\n",
                "print(f\"Timestamp: {loaded_metadata.get('timestamp', 'Unknown')}\")\n",
                "\n",
                "# Test the loaded pipeline\n",
                "test_predictions = loaded_pipeline.predict(X_test.values)\n",
                "test_accuracy_loaded = (test_predictions == y_test).mean()\n",
                "\n",
                "print(f\"\\nLoaded pipeline test accuracy: {test_accuracy_loaded:.4f}\")\n",
                "print(f\"Original model test accuracy: {test_accuracy:.4f}\")\n",
                "print(f\"Accuracy match: {abs(test_accuracy_loaded - test_accuracy) < 1e-10}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Model Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validate the saved model\n",
                "validation_results = persistence.validate_saved_model(\n",
                "    model_name=model_name,\n",
                "    test_data=(X_test.values, y_test.values)\n",
                ")\n",
                "\n",
                "print(\"Model Validation Results:\")\n",
                "print(f\"Overall validation: {'PASSED' if validation_results['is_valid'] else 'FAILED'}\")\n",
                "\n",
                "print(f\"\\nChecks passed ({len(validation_results['checks_passed'])}):\") \n",
                "for check in validation_results['checks_passed']:\n",
                "    print(f\"  ✓ {check}\")\n",
                "\n",
                "if validation_results['checks_failed']:\n",
                "    print(f\"\\nChecks failed ({len(validation_results['checks_failed'])}):\") \n",
                "    for check in validation_results['checks_failed']:\n",
                "        print(f\"  ✗ {check}\")\n",
                "\n",
                "if validation_results['warnings']:\n",
                "    print(f\"\\nWarnings ({len(validation_results['warnings'])}):\") \n",
                "    for warning in validation_results['warnings']:\n",
                "        print(f\"  ⚠ {warning}\")\n",
                "\n",
                "if 'test_performance' in validation_results:\n",
                "    perf = validation_results['test_performance']\n",
                "    print(f\"\\nTest performance: {perf['accuracy']:.4f} accuracy on {perf['n_samples']} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Create Prediction Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a prediction-optimized pipeline\n",
                "prediction_pipeline = persistence.create_prediction_pipeline(model_name)\n",
                "\n",
                "print(\"Prediction pipeline created!\")\n",
                "print(f\"Pipeline info: {prediction_pipeline.get_info()}\")\n",
                "\n",
                "# Test prediction pipeline with different input formats\n",
                "print(\"\\nTesting prediction pipeline with different input formats:\")\n",
                "\n",
                "# Test with numpy array\n",
                "sample_data = X_test.iloc[0].values\n",
                "pred_array = prediction_pipeline.predict(sample_data)\n",
                "print(f\"Numpy array input: {pred_array}\")\n",
                "\n",
                "# Test with pandas DataFrame\n",
                "sample_df = X_test.iloc[[0]]\n",
                "pred_df = prediction_pipeline.predict(sample_df)\n",
                "print(f\"DataFrame input: {pred_df}\")\n",
                "\n",
                "# Test with list\n",
                "sample_list = X_test.iloc[0].tolist()\n",
                "pred_list = prediction_pipeline.predict(sample_list)\n",
                "print(f\"List input: {pred_list}\")\n",
                "\n",
                "# Test probability predictions\n",
                "try:\n",
                "    pred_proba = prediction_pipeline.predict_proba(sample_data)\n",
                "    print(f\"Probability predictions: {pred_proba}\")\n",
                "except AttributeError as e:\n",
                "    print(f\"Probability predictions not available: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Model Versioning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create another version of the model with different parameters\n",
                "print(\"Creating a second version of the model...\")\n",
                "\n",
                "# Train a different model\n",
                "model_v2 = RandomForestClassifier(\n",
                "    n_estimators=200,\n",
                "    max_depth=15,\n",
                "    min_samples_split=2,\n",
                "    min_samples_leaf=1,\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "model_v2.fit(X_train, y_train)\n",
                "test_accuracy_v2 = model_v2.score(X_test, y_test)\n",
                "\n",
                "# Save second version\n",
                "metadata_v2 = {\n",
                "    'performance_metrics': {\n",
                "        'train_accuracy': model_v2.score(X_train, y_train),\n",
                "        'test_accuracy': test_accuracy_v2,\n",
                "        'n_train_samples': len(X_train),\n",
                "        'n_test_samples': len(X_test),\n",
                "        'n_features': X_train.shape[1]\n",
                "    },\n",
                "    'training_info': {\n",
                "        'algorithm': 'RandomForestClassifier',\n",
                "        'hyperparameters': model_v2.get_params(),\n",
                "        'training_time': datetime.now().isoformat(),\n",
                "        'data_source': 'heart_disease_dataset',\n",
                "        'version_notes': 'Increased n_estimators and max_depth'\n",
                "    },\n",
                "    'source': 'demonstration_notebook_v2'\n",
                "}\n",
                "\n",
                "saved_files_v2 = persistence.save_complete_pipeline(\n",
                "    model=model_v2,\n",
                "    preprocessing_pipeline=preprocessing_pipeline,\n",
                "    model_name=model_name,\n",
                "    metadata=metadata_v2\n",
                ")\n",
                "\n",
                "print(f\"Second version saved with test accuracy: {test_accuracy_v2:.4f}\")\n",
                "\n",
                "# Get versioning information\n",
                "version_info = persistence.model_versioning(model_name)\n",
                "\n",
                "print(f\"\\nModel Versioning Information:\")\n",
                "print(f\"Model name: {version_info['model_name']}\")\n",
                "print(f\"Total versions: {version_info['total_versions']}\")\n",
                "print(f\"Latest version: {version_info['latest_version']}\")\n",
                "print(f\"Available versions: {version_info['available_versions']}\")\n",
                "\n",
                "print(f\"\\nVersion History:\")\n",
                "for version in version_info['version_history']:\n",
                "    print(f\"  Version {version['version']}:\")\n",
                "    print(f\"    Timestamp: {version['timestamp']}\")\n",
                "    print(f\"    Model type: {version['model_type']}\")\n",
                "    print(f\"    File size: {version['file_size_mb']:.2f} MB\")\n",
                "    if version['performance']:\n",
                "        print(f\"    Performance: {version['performance']}\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Export Model for Deployment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export the latest model for deployment\n",
                "exported_files = persistence.export_model_for_deployment(\n",
                "    model_name=model_name,\n",
                "    export_format='joblib'\n",
                ")\n",
                "\n",
                "print(\"Model exported for deployment!\")\n",
                "for file_type, file_path in exported_files.items():\n",
                "    print(f\"  {file_type}: {file_path}\")\n",
                "\n",
                "# Show deployment directory contents\n",
                "deployment_dir = Path(exported_files['model']).parent\n",
                "print(f\"\\nDeployment directory contents ({deployment_dir}):\")\n",
                "for file_path in deployment_dir.iterdir():\n",
                "    if file_path.is_file():\n",
                "        size_mb = file_path.stat().st_size / (1024 * 1024)\n",
                "        print(f\"  {file_path.name}: {size_mb:.2f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Benchmark Model Loading Performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Benchmark model loading performance\n",
                "benchmark_results = persistence.benchmark_model_loading(\n",
                "    model_name=model_name,\n",
                "    n_iterations=10\n",
                ")\n",
                "\n",
                "print(\"Model Loading Benchmark Results:\")\n",
                "print(f\"Model: {benchmark_results['model_name']} v{benchmark_results['version']}\")\n",
                "print(f\"Iterations: {benchmark_results['n_iterations']}\")\n",
                "print(f\"Mean loading time: {benchmark_results['mean_loading_time']:.4f}s\")\n",
                "print(f\"Std loading time: {benchmark_results['std_loading_time']:.4f}s\")\n",
                "print(f\"Min loading time: {benchmark_results['min_loading_time']:.4f}s\")\n",
                "print(f\"Max loading time: {benchmark_results['max_loading_time']:.4f}s\")\n",
                "print(f\"Median loading time: {benchmark_results['median_loading_time']:.4f}s\")\n",
                "print(f\"Total benchmark time: {benchmark_results['total_benchmark_time']:.4f}s\")\n",
                "\n",
                "# Visualize loading times\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.hist([benchmark_results['mean_loading_time']] * 10, bins=5, alpha=0.7, color='skyblue')\n",
                "plt.axvline(benchmark_results['mean_loading_time'], color='red', linestyle='--', \n",
                "           label=f'Mean: {benchmark_results[\"mean_loading_time\"]:.4f}s')\n",
                "plt.xlabel('Loading Time (seconds)')\n",
                "plt.ylabel('Frequency')\n",
                "plt.title('Model Loading Time Distribution')\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "metrics = ['Mean', 'Median', 'Min', 'Max']\n",
                "values = [\n",
                "    benchmark_results['mean_loading_time'],\n",
                "    benchmark_results['median_loading_time'],\n",
                "    benchmark_results['min_loading_time'],\n",
                "    benchmark_results['max_loading_time']\n",
                "]\n",
                "plt.bar(metrics, values, color=['skyblue', 'lightgreen', 'orange', 'lightcoral'])\n",
                "plt.ylabel('Loading Time (seconds)')\n",
                "plt.title('Loading Time Statistics')\n",
                "plt.xticks(rotation=45)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Create Production Model Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create production-ready model files\n",
                "production_files = create_production_model(\n",
                "    persistence=persistence,\n",
                "    model_name=model_name\n",
                ")\n",
                "\n",
                "print(\"Production model files created!\")\n",
                "for file_type, file_path in production_files.items():\n",
                "    print(f\"  {file_type}: {file_path}\")\n",
                "\n",
                "# Load and display model info\n",
                "with open(production_files['model_info'], 'r') as f:\n",
                "    model_info = json.load(f)\n",
                "\n",
                "print(\"\\nProduction Model Information:\")\n",
                "print(f\"Model name: {model_info['model_name']}\")\n",
                "print(f\"Version: {model_info['version']}\")\n",
                "print(f\"Model type: {model_info['model_type']}\")\n",
                "print(f\"Production timestamp: {model_info['production_timestamp']}\")\n",
                "\n",
                "if 'performance_metrics' in model_info:\n",
                "    print(f\"Performance metrics: {model_info['performance_metrics']}\")\n",
                "\n",
                "print(f\"\\nUsage instructions:\")\n",
                "for instruction_type, instruction in model_info['usage_instructions'].items():\n",
                "    print(f\"  {instruction_type}: {instruction}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Test Production Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test loading and using the production model\n",
                "print(\"Testing production model files...\")\n",
                "\n",
                "# Load final model\n",
                "final_model = joblib.load(production_files['final_model'])\n",
                "print(f\"Final model loaded: {type(final_model).__name__}\")\n",
                "\n",
                "# Load complete pipeline\n",
                "complete_pipeline = joblib.load(production_files['complete_pipeline'])\n",
                "print(f\"Complete pipeline loaded: {type(complete_pipeline).__name__}\")\n",
                "\n",
                "# Test predictions with complete pipeline\n",
                "sample_predictions = complete_pipeline.predict(X_test.iloc[:5].values)\n",
                "actual_labels = y_test.iloc[:5].values\n",
                "\n",
                "print(\"\\nSample predictions vs actual:\")\n",
                "for i, (pred, actual) in enumerate(zip(sample_predictions, actual_labels)):\n",
                "    print(f\"  Sample {i+1}: Predicted={pred}, Actual={actual}, Match={'✓' if pred == actual else '✗'}\")\n",
                "\n",
                "# Calculate final accuracy\n",
                "final_predictions = complete_pipeline.predict(X_test.values)\n",
                "final_accuracy = (final_predictions == y_test.values).mean()\n",
                "print(f\"\\nFinal production model accuracy: {final_accuracy:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. Summary and Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a comprehensive summary visualization\n",
                "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
                "fig.suptitle('Model Persistence System Summary', fontsize=16, fontweight='bold')\n",
                "\n",
                "# 1. Model versions comparison\n",
                "versions = version_info['available_versions']\n",
                "accuracies = [test_accuracy, test_accuracy_v2]  # Assuming we have 2 versions\n",
                "\n",
                "axes[0, 0].bar(range(len(versions)), accuracies, color=['skyblue', 'lightgreen'])\n",
                "axes[0, 0].set_xlabel('Model Version')\n",
                "axes[0, 0].set_ylabel('Test Accuracy')\n",
                "axes[0, 0].set_title('Model Versions Performance')\n",
                "axes[0, 0].set_xticks(range(len(versions)))\n",
                "axes[0, 0].set_xticklabels([f'v{v}' for v in versions])\n",
                "axes[0, 0].grid(True, alpha=0.3)\n",
                "\n",
                "# Add accuracy values on bars\n",
                "for i, acc in enumerate(accuracies):\n",
                "    axes[0, 0].text(i, acc + 0.01, f'{acc:.4f}', ha='center', va='bottom')\n",
                "\n",
                "# 2. File sizes\n",
                "file_types = ['Pipeline', 'Model', 'Metadata']\n",
                "file_sizes = []\n",
                "for version in version_info['version_history']:\n",
                "    file_sizes.append(version['file_size_mb'])\n",
                "    break  # Just show the latest version\n",
                "\n",
                "# Add dummy sizes for visualization\n",
                "file_sizes = [file_sizes[0] if file_sizes else 1.0, 0.8, 0.1]\n",
                "\n",
                "axes[0, 1].pie(file_sizes, labels=file_types, autopct='%1.1f%%', startangle=90)\n",
                "axes[0, 1].set_title('Model File Size Distribution')\n",
                "\n",
                "# 3. Loading performance\n",
                "loading_metrics = ['Mean', 'Median', 'Min', 'Max']\n",
                "loading_values = [\n",
                "    benchmark_results['mean_loading_time'],\n",
                "    benchmark_results['median_loading_time'],\n",
                "    benchmark_results['min_loading_time'],\n",
                "    benchmark_results['max_loading_time']\n",
                "]\n",
                "\n",
                "bars = axes[1, 0].bar(loading_metrics, loading_values, \n",
                "                     color=['skyblue', 'lightgreen', 'orange', 'lightcoral'])\n",
                "axes[1, 0].set_ylabel('Loading Time (seconds)')\n",
                "axes[1, 0].set_title('Model Loading Performance')\n",
                "axes[1, 0].grid(True, alpha=0.3)\n",
                "\n",
                "# Add values on bars\n",
                "for bar, value in zip(bars, loading_values):\n",
                "    axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.0001, \n",
                "                   f'{value:.4f}s', ha='center', va='bottom')\n",
                "\n",
                "# 4. Validation results\n",
                "validation_categories = ['Passed', 'Failed', 'Warnings']\n",
                "validation_counts = [\n",
                "    len(validation_results['checks_passed']),\n",
                "    len(validation_results['checks_failed']),\n",
                "    len(validation_results['warnings'])\n",
                "]\n",
                "\n",
                "colors = ['green', 'red', 'orange']\n",
                "bars = axes[1, 1].bar(validation_categories, validation_counts, color=colors, alpha=0.7)\n",
                "axes[1, 1].set_ylabel('Number of Checks')\n",
                "axes[1, 1].set_title('Model Validation Results')\n",
                "axes[1, 1].grid(True, alpha=0.3)\n",
                "\n",
                "# Add values on bars\n",
                "for bar, count in zip(bars, validation_counts):\n",
                "    if count > 0:\n",
                "        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
                "                       str(count), ha='center', va='bottom')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Print final summary\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"MODEL PERSISTENCE DEMONSTRATION COMPLETED SUCCESSFULLY!\")\n",
                "print(\"=\"*70)\n",
                "print(f\"✓ Created and saved {version_info['total_versions']} model versions\")\n",
                "print(f\"✓ Validated model integrity with {len(validation_results['checks_passed'])} checks passed\")\n",
                "print(f\"✓ Created production-ready model files\")\n",
                "print(f\"✓ Exported deployment package\")\n",
                "print(f\"✓ Benchmarked loading performance: {benchmark_results['mean_loading_time']:.4f}s average\")\n",
                "print(f\"✓ Final model accuracy: {final_accuracy:.4f}\")\n",
                "print(\"\\nAll model persistence features demonstrated successfully!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
